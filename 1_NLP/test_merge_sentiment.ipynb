{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 64-bit' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: 'c:/Users/attawut/AppData/Local/Programs/Python/Python39/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import emoji\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "def remove_url(txt):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'',txt)\n",
    "\n",
    "def remove_html(txt):\n",
    "    html=re.compile(r'<.*?>')\n",
    "    return html.sub(r'',txt)\n",
    "\n",
    "# U+1F970\n",
    "def remove_emoji(txt):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', txt)\n",
    "def remove(emoji):\n",
    "    em = re.compile(r\"ðŸ¥°\")\n",
    "    return em.sub(r\"\",emoji)\n",
    "def remove_blank_space(text):\n",
    "    return re.sub(\"\\s\\s+\" , \" \", text)\n",
    "def remove_blank2(text):\n",
    "    text = text.strip()\n",
    "    return text\n",
    "def remove_all(ReviewText):\n",
    "    ReviewText = ReviewText.replace(\"(<br/>)\", \"\")\n",
    "    ReviewText = ReviewText.replace('(<a).*(>).*(</a>)', '')\n",
    "    ReviewText = ReviewText.replace('(&amp)', '')\n",
    "    ReviewText = ReviewText.replace('(&gt)', '')\n",
    "    ReviewText = ReviewText.replace('(&lt)', '')\n",
    "    ReviewText = ReviewText.replace('(\\xa0)', ' ') \n",
    "    ReviewText = ReviewText.replace(r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', ' ') \n",
    "    return ReviewText\n",
    "\n",
    "from wordcloud import STOPWORDS\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "s_words= list(stopwords.words('english'))\n",
    "stop_words = list(STOPWORDS)+ [\"what\", \"us\", \"this\",\"well\",\"there\",\"much\",\"us\",\"and\",\"you're\",\"in\",\"where\",\"when\",\"just\",\"how\",\"is\",\"ha\",\"re\",\"are\"\n",
    "                              \"hi\",\"aren't\", 'couldn','could','couldnt',\"couldn't\",'did','had','have','must','does','should','was',\"it's\"\n",
    "                               \"didn't\", \"doesn't\", \"don't\", \"hadn't\", \"hasn't\", \"haven't\", \"isn't\", 'let', 'll',\"may\",'were','is','has','must',\n",
    "                               'mustn', 'rt', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn','realli','now','got','man','people','a',\n",
    "                               'becaus','caus',\"one\",\"im\",\"guy\",\"someone\",\"two\",\"nearby\",\"i\",\"he's\",\"she's\",\"we\",\"it\",\"they\",\"wouldnâ€™t\",\"i've\",\n",
    "                               'aren', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'sdidn', 've',\"will\",\"restaurant\"]\n",
    "\n",
    "\n",
    "\n",
    "def remove_stopwords(txt):\n",
    "    \n",
    "    sentence = []\n",
    "    s = \"\"\n",
    "    for word in txt.split():    \n",
    "        if(word not in stop_words):      \n",
    "            sentence.append(word)\n",
    "            s = ' '.join(sentence)\n",
    "    return s\n",
    "\n",
    "def strip_emoji(text):\n",
    "\n",
    "    print(emoji.emoji_count(text))\n",
    "\n",
    "    new_text = re.sub(emoji.get_emoji_regexp(), r\"\", text)\n",
    "\n",
    "    return new_text\n",
    "\n",
    "## Creating sentimental polarity \n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "def compound_score(txt):\n",
    "    return analyzer.polarity_scores(txt)[\"compound\"]\n",
    "\n",
    "## Sentiments\n",
    "def sentiment(score):\n",
    "    emotion = \"\"\n",
    "    if score >= 0.5:\n",
    "        emotion = \"Positive\"\n",
    "    elif score <= -0.5:\n",
    "        emotion = \"Negative\"\n",
    "    else:\n",
    "        emotion = \"Neutral\"\n",
    "    return emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import words\n",
    "from stop_words import get_stop_words\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "atm_data = []\n",
    "header_list = [\"1\", \"2\", \"3\",\"4\",\"5\"]\n",
    "# create a Path instance and filter for only csv files\n",
    "files = Path(\"input_data/\").rglob(\"*.csv\") #Input Folder à¸—à¸µà¹ˆà¸—à¸³à¸à¸²à¸£ Rename à¹€à¸£à¸µà¸¢à¸šà¸£à¹‰à¸­à¸¢à¹à¸¥à¹‰à¸§\n",
    "\n",
    "for i in files:\n",
    "    df = pd.read_csv(i, names=header_list)\n",
    "    df1 = df['Review'].str.cat(sep='')\n",
    "    df2 = strip_emoji(df1)\n",
    "    df3 = clean_msg(df2)\n",
    "    df4 = split_word(df3)\n",
    "    df5 = listToString(df4)\n",
    "    result = get_sentiment(df5)\n",
    "    names = i.name\n",
    "    results = result\n",
    "    data = [names, results]\n",
    "    atm_data.append(data)\n",
    "    print(names)\n",
    "    \n",
    "labels = ['Name','result'] \n",
    "data_result = pd.DataFrame.from_records(atm_data, columns=labels)\n",
    "data_result.to_csv('tripadvisor_restaurant_sentiment_28092021.csv',encoding='utf-8-sig') #à¸œà¸¥à¸¥à¸±à¸žà¸˜à¹Œà¸—à¸µà¹ˆà¹„à¸”à¹‰à¸„à¸£à¸±à¸š\n",
    "print(\"Finish!!\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Review = df.Review.apply(remove_url) #à¹ƒà¸Šà¹‰ method à¹€à¸¥à¸·à¸­à¸ column à¸—à¸µà¹ˆà¹€à¸à¹‡à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸£à¸µà¸§à¸´à¸§\n",
    "df.Review = df.Review.apply(remove_html) #à¹ƒà¸Šà¹‰ method à¹€à¸¥à¸·à¸­à¸ column à¸—à¸µà¹ˆà¹€à¸à¹‡à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸£à¸µà¸§à¸´à¸§\n",
    "df.Review = df.Review.apply(remove_emoji) #à¹ƒà¸Šà¹‰ method à¹€à¸¥à¸·à¸­à¸ column à¸—à¸µà¹ˆà¹€à¸à¹‡à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸£à¸µà¸§à¸´à¸§\n",
    "df.Review = df.Review.apply(remove) #à¹ƒà¸Šà¹‰ method à¹€à¸¥à¸·à¸­à¸ column à¸—à¸µà¹ˆà¹€à¸à¹‡à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸£à¸µà¸§à¸´à¸§\n",
    "df.Review = df.Review.apply(remove_blank_space) #à¹ƒà¸Šà¹‰ method à¹€à¸¥à¸·à¸­à¸ column à¸—à¸µà¹ˆà¹€à¸à¹‡à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸£à¸µà¸§à¸´à¸§\n",
    "df.Review = df.Review.apply(remove_blank2) #à¹ƒà¸Šà¹‰ method à¹€à¸¥à¸·à¸­à¸ column à¸—à¸µà¹ˆà¹€à¸à¹‡à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸£à¸µà¸§à¸´à¸§\n",
    "df.Review = df.Review.apply(remove_all) #à¹ƒà¸Šà¹‰ method à¹€à¸¥à¸·à¸­à¸ column à¸—à¸µà¹ˆà¹€à¸à¹‡à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸£à¸µà¸§à¸´à¸§\n",
    "df.Review = df.Review.map(remove_stopwords) #à¹ƒà¸Šà¹‰ method à¹€à¸¥à¸·à¸­à¸ column à¸—à¸µà¹ˆà¹€à¸à¹‡à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸£à¸µà¸§à¸´à¸§\n",
    "df.Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "atm_data = []\n",
    "header_list = [\"1\", \"2\", \"3\",\"4\",\"5\"]\n",
    "# create a Path instance and filter for only csv files\n",
    "files = Path(\"input_data/\").rglob(\"*.csv\") #Input Folder à¸—à¸µà¹ˆà¸—à¸³à¸à¸²à¸£ Rename à¹€à¸£à¸µà¸¢à¸šà¸£à¹‰à¸­à¸¢à¹à¸¥à¹‰à¸§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_data\\mining_2fortheroadà¸«à¸²à¸”à¸ˆà¸­à¸¡à¹€à¸—à¸¢à¸™à¹„à¸—à¸¢.csv\n",
      "input_data\\mining_2à¸ªà¸à¸²à¸¢à¸žà¸—à¸¢à¸²à¸£à¸­à¸„à¹€à¸à¸•à¸šà¸­à¸¥à¹„à¸—à¸¢.csv\n",
      "input_data\\mining_313TrainingStudioà¸žà¸—à¸¢à¸²à¹„à¸—à¸¢.csv\n",
      "input_data\\mining_4TBarà¸žà¸—à¸¢à¸²à¹„à¸—à¸¢.csv\n",
      "input_data\\mining_7à¸ªà¸›à¸²à¸¡à¸²à¸Šà¸²à¸¥à¸à¸‹à¸£à¸žà¸—à¸¢à¸²à¹„à¸—à¸¢.csv\n",
      "input_data\\mining_808Clubà¸žà¸—à¸¢à¸²à¹„à¸—à¸¢.csv\n",
      "input_data\\mining_AbbesBarà¸žà¸—à¸¢à¸²à¹„à¸—à¸¢.csv\n",
      "input_data\\mining_AdventureDiversPattayaà¸žà¸—à¸¢à¸²à¹„à¸—à¸¢.csv\n",
      "input_data\\mining_AIRPORTELsMakepeopleTravelLightAnywhereà¸žà¸—à¸¢à¸²à¹„à¸—à¸¢.csv\n",
      "input_data\\mining_AisawanSpaà¸žà¸—à¸¢à¸²à¹„à¸—à¸¢.csv\n",
      "input_data\\mining_ALaCampagneà¹€à¸¡à¸­à¸‡à¸Šà¸¥à¸šà¸£à¹„à¸—à¸¢.csv\n",
      "input_data\\mining_AmazonFishingParkPattayaà¸žà¸—à¸¢à¸²à¹„à¸—à¸¢.csv\n",
      "input_data\\mining_AmburayaSpaà¸žà¸—à¸¢à¸²à¹„à¸—à¸¢.csv\n",
      "input_data\\mining_AquanautsDiveCenterà¸žà¸—à¸¢à¸²à¹„à¸—à¸¢.csv\n",
      "input_data\\mining_AREEMassageà¸žà¸—à¸¢à¸²à¹„à¸—à¸¢.csv\n",
      "input_data\\mining_AromdeeMassageà¸žà¸—à¸¢à¸²à¹„à¸—à¸¢.csv\n",
      "input_data\\mining_ARTmassageà¸žà¸—à¸¢à¸²à¹„à¸—à¸¢.csv\n",
      "input_data\\mining_ArtStreetPattayaà¸žà¸—à¸¢à¸²à¹„à¸—à¸¢.csv\n",
      "input_data\\mining_ArttoArtGalleryà¸žà¸—à¸¢à¸²à¹„à¸—à¸¢.csv\n",
      "input_data\\mining_AshtangaYogaPattayaà¸žà¸—à¸¢à¸²à¹„à¸—à¸¢.csv\n"
     ]
    }
   ],
   "source": [
    "for i in files:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eb4f3c2745d5e24f01fa2041e6a45ad2b07924bca0b56c56c7d2e71a35823404"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
